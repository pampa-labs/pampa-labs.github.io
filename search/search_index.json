{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Pampa Labs Technical Blog: Your LLM Expertise Hub","text":"<p>Welcome to the Pampa Labs Technical Blog, your go-to resource for becoming proficient in Large Language Model (LLM) technologies. Our focus is on providing practical, hands-on knowledge to help you navigate the complex world of LLMs.</p>"},{"location":"#our-llm-centric-approach","title":"Our LLM-Centric Approach","text":"<p>At Pampa Labs, we understand that the key to mastering LLMs lies in practical application and technical know-how. Our blog is dedicated to delivering actionable insights and expert tips that will elevate your LLM skills from novice to native. We cut through the theoretical noise to bring you:</p> <ul> <li>Step-by-step guides for implementing LLM solutions</li> <li>Best practices for optimizing LLM performance</li> <li>Real-world case studies showcasing successful LLM integrations</li> <li>Troubleshooting tips for common LLM challenges</li> <li>Insider techniques for enhancing LLM outputs</li> <li>Practical ways to use LLM tools to improve your daily workflows as a developer</li> </ul> <p>Our content is crafted by experienced LLM practitioners who understand the nuances and complexities of working with these powerful models. We focus on the practical aspects that matter most in day-to-day LLM development and deployment.</p>"},{"location":"#become-llm-native-with-us","title":"Become LLM-Native with Us","text":"<p>Whether you're looking to fine-tune your prompt engineering skills, optimize your model's performance, or tackle advanced LLM applications, our technical articles provide the concrete, applicable knowledge you need. We're committed to helping you become truly LLM-native, equipped with the skills to leverage these technologies effectively in your projects and workflows.</p> <p>Join us as we delve into the nitty-gritty of LLM technology, offering the practical tips and insights you need to thrive in this rapidly evolving field.</p>"},{"location":"technical-writings/","title":"Technical Writings","text":""},{"location":"technical-writings/2024/11/12/ais-first-day-on-the-job---exposing-the-gaps/","title":"AI\u2019s First Day on the Job - Exposing the Gaps","text":"<p>Bringing AI into a company is like hiring a new employee \u2014 one that\u2019s demanding, detail-oriented, and can\u2019t rely on intuition. AI, like any new team member, needs everything explained clearly and thoroughly. </p> <p>This process often reveals that our knowledge isn\u2019t as well-structured or organized as we thought. Bringing AI into our operations makes us take a closer look at our processes and documentation, showing us gaps and inefficiencies we might be missing.</p>"},{"location":"technical-writings/2024/11/12/ais-first-day-on-the-job---exposing-the-gaps/#when-ai-joins-your-team-get-ready-for-a-reality-check","title":"When AI Joins Your Team, Get Ready for a Reality Check","text":"<p>To use AI effectively, companies need standardized processes and comprehensive documentation. Many fall short here, internal documentation often lacks the necessary rigor because it\u2019s considered \u201cgood enough\u201d for internal use \u2014 outdated, imprecise, and full of gaps that only certain employees can fill with their personal knowledge. This might work in a human-only environment where employees can adapt and fill in these gaps intuitively, but for AI, imprecision is not an option.</p>"},{"location":"technical-writings/2024/11/12/ais-first-day-on-the-job---exposing-the-gaps/#time-to-clean-the-house","title":"Time to Clean the House","text":"<p>Preparing documentation for AI forces companies to raise their standards across the board, ultimately benefiting not only the AI but also every team member who relies on this information. A well-documented, standardized knowledge base helps everyone operate more effectively, reduces misunderstandings, and creates a more seamless flow of information throughout the organization.</p> <p>Integrating AI forces us to confront the disorganization we\u2019ve ignored for years. It highlights how much of our operational knowledge is informal and reliant on individuals rather than being part of a structured system. Many organizations depend heavily on the tacit knowledge of key employees \u2014 information that is undocumented and inaccessible to others. </p>"},{"location":"technical-writings/2024/11/12/ais-first-day-on-the-job---exposing-the-gaps/#no-knowledge-superheroes-allowed","title":"No Knowledge Superheroes Allowed","text":"<p>Humans are adaptable; we can work around inefficiencies and navigate ambiguities with ease. However, AI does not have this luxury \u2014 it demands clarity and precision. Addressing this need for clarity doesn\u2019t just help AI; it helps everyone within the organization. By making processes more explicit and reducing reliance on informal knowledge, we create a more robust and reliable working environment for all employees, ensuring that critical knowledge is available when needed, regardless of who left the team last week.</p>"},{"location":"technical-writings/2024/11/12/ais-first-day-on-the-job---exposing-the-gaps/#lets-get-real-nobody-reads-documentation","title":"Let's Get Real, Nobody Reads Documentation","text":"<p>Let's face it - even with well-organized documentation, humans rarely read it thoroughly. But this is where AI changes everything. Instead of searching through documents, we're moving towards AI-indexing \u2014 where AI can directly quote from our documentation to give immediate, precise answers. This makes all that standardized documentation we created not just accessible, but highly useful in daily operations.</p> <p>AI is not just a tool \u2014 it is the 'new hire' that shakes things up, questions our habits, and demands transparency. By challenging the way we do things, it exposes the flaws and gaps in our processes and pushes us to build a stronger foundation.</p> <p>Yes, it is uncomfortable to face disorganization debt. Yes, it requires effort and a willingness to change. But becoming LLM-native is the opportunity we have been needing to elevate our standards, sharpen our processes, and take our business to the next level.</p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/","title":"Toward an Age of Interoperable Agents","text":"<p>The great AI unbundling is marking a shift from one-size-fits-all solutions to a LEGO-like world where developers can pick the best AI models and tools, mixing and matching components to build systems that perfectly fit their needs.</p> <p></p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#the-great-unbundling","title":"The Great Unbundling","text":"<p>The year 2023 marked the peak of monolithic AI frameworks. These all-in-one solutions gradually evolved to encompass all AI development needs under a single roof. They were convenient, comprehensive, and, as many teams discovered, increasingly constraining.</p> <p>Fast forward to the beggining of 2025, and the landscape is increasingly different. Organizations are dismantling these structures in favor of something more flexible: modular ecosystems where each component is interchangeable and best-in-class.</p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#the-modular-ecosystem","title":"The Modular Ecosystem","text":""},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#core-building-blocks","title":"Core Building Blocks","text":"<p>More and more AI systems are increasingly adopting modular architectures where components connect through standard interfaces. Foundation models serve as the cognitive engine, ranging from general-purpose large language models to specialized models for specific domains or tasks. These models are complemented by a rich ecosystem of tools and integrations that handle everything from data processing to action execution.</p> <p>The orchestration layer has evolved to support different levels of abstraction, allowing organizations to choose the right balance of control and convenience. High-level orchestrators provide simplified interfaces for common patterns, while low-level frameworks offer granular control over system behavior. This flexibility enables teams to adapt their architecture as their needs evolve.</p> <p>Today's AI agents are no longer singular towers of functionality. They are built from modular parts\u2014each designed to be interchangeable. These building blocks include:</p> <ul> <li>Architectures: The core design that integrates and connects all components, providing a framework for the agent.</li> <li>Models (LLMs): Large language models like those from OpenAI and Anthropic, which provide capabilities for understanding and generating language (and other modalities)</li> <li>Tools and Resources: Components that extend functionality, from accessing databases to integrating applications and performing actions in the physical world.</li> <li>Prompts: Configurable inputs that direct models to generate specific outputs, allowing developers to control AI behavior.</li> </ul> <p>This modularity is revolutionizing AI development, allowing developers to swap, upgrade, or fine-tune parts without overhauling entire systems.</p> <p></p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#a-common-language","title":"A Common Language","text":"<p>Interoperability protocols are key to making these diverse parts work together. Before such protocols, integrating new models or tools required custom-built solutions. Protocols like Anthropic's Model Context Protocol (MCP) provide a common language for all components to communicate.</p> <p>With standardized protocols like MCP, adding a new model or replacing an outdated tool becomes straightforward. These protocols reduce friction, allowing developers to innovate and reduce reliance on exclusive integrations.</p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#the-rise-of-marketplaces-and-ecosystems","title":"The Rise of Marketplaces and Ecosystems","text":"<p>Each building block for LLMs is evolving into a rich marketplace. Language models, tools, and observability frameworks  compete on quality, cost, and features. Developers increasingly have the freedom to mix and match parts without compatibility concerns.</p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#llm-providers","title":"LLM Providers","text":"<p>On the LLM provider side, we have tools which allow transparent switching between different LLM providers without any change in the code besides the name of the model. We also have general routers like Not Diamond that help developers dynamically select the best model for the input at hand (even tuned to their specific use case). This also serves to progressively highlight great open-source models that might be highly specialized for specific tasks but aren't widely visible to a wider audience.</p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#tools-and-prompts","title":"Tools and prompts","text":"<p>Tools are also moving towards standardized, templated blocks that can be seamlessly integrated into various architectures. This evolution means developers no longer need frameworks with built-in tool support, instead plugging custom agents with tools from diverse providers. Utilities like the Arcade SDK exemplify this trend by allowing devs to effortlessly build custom tooling with built-in authentication, helping users have easy access to utilities like Slack, Calendar or Mail.</p> <p>On the prompt side, we have several platforms that facilitate the sharing, versioning, and discovery of prompt templates, enabling developers to collaborate and access a diverse array of prompts for various use cases. </p> <p>By leveraging tools like these, developers can treat prompts as modular components\u2014starting with high-performing templates and refining them into tailored, use case-specific prompts optimized for their internal evaluation datasets.</p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#observability","title":"Observability","text":"<p>In what regards to observability frameworks, many of the most popular ones like Langsmith, W&amp;B Weave, and Langfuse rely on a decorator-based approach, which simplifies their integration to any agent codebase and makes them easily interchangeable.</p> <p>With decreasing switching costs, developers now have the freedom to choose among a wide array of LLMs, tools, and observability frameworks, using them interchangeably to fit their specific needs. </p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#frameworks","title":"Frameworks","text":"<p>With modular building blocks and standardized protocols, frameworks are evolving from being component integrators to agent orchestrators. These frameworks serve a spectrum of developers, from those seeking simplicity to seasoned developers looking for advanced customization options.</p> <p>High-level frameworks like CrewAI streamline the creation of AI agent teams by abstracting complexity, while lower level frameworks like LangGraph offer granular control over agent behavior through a graph-based architecture. This lower level flexibility enables experienced developers to define and orchestrate agent workflows with fine-grained management of task dependencies.</p> <p>Regardless of whether they prioritize flexibility or simplicity, frameworks increasingly stand on their orchestration merits - developers now choose them based on their core strengths in building agents, not their bundled components.</p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#decentralization-as-the-new-norm","title":"Decentralization as the New Norm","text":"<p>The modular approach, enabled by standardized protocols and interchangeable parts, is decentralizing the AI ecosystem. Instead of waiting for each component to be designed for compatibility with their primary framework, developers can freely combine independent pieces from anywhere. This shift from built-in compatibility to universal protocols means innovation happens in parallel - new tools and models can emerge and be adopted immediately without needing for frameworks to catch up.</p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#conclusion-toward-modularity","title":"Conclusion: Toward modularity","text":"<p>Modular AI ecosystems are transforming the way we build intelligent systems. Teams can select and combine specialized components based on their unique strengths. This shift from monolithic solutions to interchangeable components marks the beginning of truly customizable AI systems.</p> <p>The path forward isn't without its obstacles. Each additional component introduces latency. Models interpret instructions differently. Systems become harder to test. Yet these challenges pale in comparison to the increased flexibility and iteration speed. As agent systems grow more complex, the ability to mix and match specialized components is less of a luxury and more of a necessity.</p>"},{"location":"technical-writings/2025/01/27/toward-an-age-of-interoperable-agents/#references","title":"References","text":"<ul> <li>Model Context Protocol (MCP) </li> <li>Not Diamond </li> <li>Arcade SDK </li> <li>LangSmith </li> <li>Langfuse </li> <li>W&amp;B Weave</li> <li>LangGraph</li> <li>CrewAI</li> </ul>"},{"location":"technical-writings/2024/10/29/a-truth-your-domain-experts-cant-ignore/","title":"A Truth Your Domain Experts Can\u2019t Ignore","text":"<p>Want your AI to deliver accurate answers every time? There's a critical element many teams overlook.</p> <p>A Ground Truth Dataset is your foundation for reliable LLM evaluation. While some metrics don't need it, having this human-validated reference data is essential for truly understanding if your AI is performing as intended. Read on to discover why your Subject Matter Experts need to care about Ground Truth, and how to build it right.</p>"},{"location":"technical-writings/2024/10/29/a-truth-your-domain-experts-cant-ignore/#ground-truth-is-not-optional","title":"Ground Truth is not optional","text":"<p>Let's talk about something that\u2019s often treated as optional, when it really shouldn\u2019t be: having a solid Ground Truth is as necessary as defining a project roadmap. </p> <p>You wouldn\u2019t kick off a project without knowing what success looks like, so why would you launch an LLM-based project without knowing what correct answers look like? </p> <p>The Ground Truth is what defines how you want your agent to respond and the specific standards you are aiming for. Without it, how can you measure your progress or even know if your solution is hitting the mark?</p>"},{"location":"technical-writings/2024/10/29/a-truth-your-domain-experts-cant-ignore/#no-vibe-check-allowed","title":"No vibe check allowed","text":"<p>We've seen many projects where people skip this step because it\u2019s challenging or costly\u2014requiring involvement from subject matter experts (SMEs) and the broader team. But trust me, building a well-defined Ground Truth dataset is a must from the start. Yes, it\u2019s resource-intensive, but it\u2019s also foundational. </p> <p>Without it, you\u2019re flying blind, relying only on a vibe check or intuition about whether the LLM is doing well. And that\u2019s just not enough...</p>"},{"location":"technical-writings/2024/10/29/a-truth-your-domain-experts-cant-ignore/#ground-truth-must-be-true","title":"Ground Truth must be true","text":"<p>So, what does a good Ground Truth look like? When working with SMEs to create a Ground Truth dataset, you should expect them to provide:</p> <ol> <li>Essential information elements that must be present in any correct response</li> <li>Acceptable variations in how information can be presented</li> <li>Clear guidelines on tone and style requirements</li> </ol> <p>Given the probabilistic nature of LLMs, we can't expect them to produce identical answers every time. However, your experts should clearly define what constitutes a valid response, including both content requirements and presentation standards. This becomes your Ground Truth - not rigid, word-for-word templates, but rather a comprehensive specification that ensures consistency and accuracy across all responses. </p> <p>Let's look at the examples below:</p> <p>Input prompt: What are the main features of the premium membership plan?</p> <ul> <li>Poor Ground Truth: \u201cThe premium plan offers lots of great features and benefits that make it a good choice for many users.\u201d</li> <li>Good Ground Truth: \u201cThe premium membership plan provides exclusive benefits, including unlimited access to resources, personalized support, priority booking, and additional discounts on services \u2014 designed to enhance your experience at every step!. [Source: Membership Policy Document, Section 3.1]\u201d</li> </ul>"},{"location":"technical-writings/2024/10/29/a-truth-your-domain-experts-cant-ignore/#ground-truth-is-a-joint-effort","title":"Ground Truth is a joint effort","text":"<p>Starting an LLM project requires defining what the system should and shouldn't do, while planning how to create and validate the Ground Truth data needed to achieve these goals.</p> <p>Here's the deal: Your SMEs don't need a PhD in AI \u2014 but they do need to grasp what makes Ground Truth tick. Think of it like teaching someone to drive: they don't need to know how the engine works, but they need to understand the rules of the road. Let's show them exactly how their expertise fits into the bigger picture. </p> <p>You should aim to have something like this:</p> <p></p>"},{"location":"technical-writings/2024/10/29/a-truth-your-domain-experts-cant-ignore/#it-is-not-a-solo-sme-job","title":"It is not a solo SME job","text":"<p>Ground Truth creation is a collaborative process that should involve both SMEs and AI engineers, working together in a well-oiled loop. The SME's role is to provide all the necessary information for a good response, while it's the responsibility of the science team to achieve this in practice. Any updates to the documentation or changes to Ground Truth should be easy to implement across the system. </p>"},{"location":"technical-writings/2024/10/29/a-truth-your-domain-experts-cant-ignore/#using-synthetic-data","title":"Using Synthetic Data","text":"<p>A tip I\u2019d like to share: if you don\u2019t yet have validated Ground Truth, definitely consider starting with synthetic data. </p> <p>Generate questions from your existing documentation and have an LLM provide theoretical answers\u2014these serve as initial synthetic Ground Truths, and is better than having nothing as a reference. </p>"},{"location":"technical-writings/2024/10/29/a-truth-your-domain-experts-cant-ignore/#just-a-starting-point","title":"Just a starting point","text":"<p>This approach can be useful for setting up a baseline evaluation suite without relying entirely on SMEs from day one. But remember, synthetic data is a starting point, not an end goal. It\u2019s tempting to stick with it, but you need to move on to real Ground Truths validated by SMEs. </p>"},{"location":"technical-writings/2024/10/29/a-truth-your-domain-experts-cant-ignore/#avoid-these-common-pitfalls","title":"Avoid These Common Pitfalls","text":"<ul> <li> <p>Overburdening SMEs: Don't expect SMEs to become LLM experts. Instead, create structured processes where they focus solely on domain expertise while AI engineers handle technical aspects.</p> </li> <li> <p>Dependence on Synthetic Data: While synthetic data helps get started, it can introduce biases and make transitioning to human validation difficult. Use it strategically and plan for a gradual shift to human-validated data.</p> </li> <li> <p>Stuck in the Iteration Loop?: Don't let ground truth updates become a bottleneck! Keep your momentum by using version control, breaking changes into bite-sized pieces, focusing on high-impact updates first.</p> </li> </ul> <p>Think of Ground Truth as your LLM project's compass \u2013 get it right, and everything else falls into place. Sure, establishing rock-solid Ground Truth might feel like finding true north at first, but every step in the right direction translates directly into an LLM that can be trusted. </p> <p>Your future self (and your users!) will thank you for putting in the work today.</p>"},{"location":"technical-writings/2025/01/20/connecting-mcp-servers-to-langgraph/","title":"Connecting MCP Servers to LangGraph","text":"<p>In this article, I'll take you through the step-by-step process of building an MCP server and connecting it to a custom solution on LangGraph (client).</p> <p></p>"},{"location":"technical-writings/2025/01/20/connecting-mcp-servers-to-langgraph/#introduction","title":"Introduction","text":"<p>At the last MCP hackathon, I had the opportunity to delve into the Model Context Protocol (MCP), a transformative framework introduced by Anthropic in December 2024. With its flexible and extensible architecture, MCP not only simplifies but also standardizes communication between LLM applications and integrations providing developers with a seamless and efficient foundation for building advanced actors.</p>"},{"location":"technical-writings/2025/01/20/connecting-mcp-servers-to-langgraph/#mcp-architecture-overview","title":"MCP Architecture Overview","text":"<p>MCP operates on a client-server model, offering a robust framework where host applications seamlessly connect to multiple servers. Like REST standardizes web APIs, MCP is doing the same for LLMs\u2014acting as a translator that bridges and standardizes tools for LLM frameworks/apps.</p> <ul> <li>MCP Hosts: Applications like Claude Desktop or IDEs that initiate connections. In this article, I'll demonstrate how to integrate a LangGraph solution to act as a client.</li> <li>MCP Clients: Protocol clients that manage direct communication with servers.</li> <li>MCP Servers: Lightweight programs designed to provide specific capabilities through the standardized protocol. These servers act as \"Actors,\" delivering context, tools, and prompts to clients.</li> </ul> <p></p> <p>In my opinion, MCP unifies the fragmented ecosystem by solving common developer issues when they want to build agents. It eliminates the need to rewrite code for different languages, like Python and TypeScript, reduces maintenance headaches from adapting tools across frameworks, and saves time on bespoke solutions for integrating tools like scrapers with LLMs. With pre-built integrations, language-agnostic tools, and streamlined connections, MCP makes development faster and more efficient.</p> <p>Let's dive into the implementation of an MCP server and how to connect it to LangGraph.</p>"},{"location":"technical-writings/2025/01/20/connecting-mcp-servers-to-langgraph/#mcp-server-implementation","title":"MCP Server Implementation","text":"<p>An example of an MCP server in action is this website researcher service that I built to transform any website into structured, relevant content based on user queries. By combining Firecrawl's mapping, selection, and scraping features with GPT-4's intelligent URL filtering, this server functions as an AI-powered research assistant, predicting and fulfilling user needs effectively.</p> <p></p> <p>To achieve this, I implemented the two primary functions: one for defining the schema (<code>handle_list_tools</code>) and another for managing the tool's logic (<code>handle_call_tool</code>):</p> <pre><code>server = Server(\"website_firecrawl_service\")\n\nclass WebsiteCrawlArgs(BaseModel):\n    \"\"\"\n    Arguments for crawling a website\n    \"\"\"\n    query: str\n    base_url: str\n    max_links: int = 100\n\n    model_config = {\n        \"json_schema_extra\": {\n            \"description\": \"Arguments for crawling a website\"\n        }\n    }\n\n@server.list_tools()\nasync def handle_list_tools() -&gt; list[types.Tool]:\n    \"\"\"\n    List available tools.\n    Each tool specifies its arguments using JSON Schema validation.\n    \"\"\"\n    return [\n        types.Tool(\n            name=\"website_firecrawl\",\n            description=\"Crawl a website\",\n            inputSchema=WebsiteCrawlArgs.model_json_schema(),\n        )\n    ]\n\n@server.call_tool()\nasync def handle_call_tool(\n    name: str, arguments: Optional[dict]\n) -&gt; List[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n    \"\"\"\n    Handle tool execution requests.\n\n    Args:\n        name: The name of the tool to execute\n        arguments: Dictionary of tool arguments\n\n    Returns:\n        List of content items produced by the tool\n    \"\"\"\n\n        if name != \"website_firecrawl\":\n            raise ValueError(f\"Unknown tool: {name}\")\n\n        if not arguments:\n            raise ValueError(\"Missing arguments\")\n\n        args = WebsiteCrawlArgs.model_validate(arguments)\n\n        #The Tool Logic\n\n        return [\n            types.TextContent(\n                type=\"text\",\n                text=response\n            )\n        ]\n</code></pre> <p>Then, I just needed to run the server with the following command, and the server would be available to consume from any MCP client:</p> <pre><code>uv run website_firecrawl_service\n</code></pre> <p>You can view the complete implementation in the MCP Server repository. It is compatible with any MCP client you choose, such as Claude Desktop, Langgraph, or any other MCP client.</p> <p></p> <p>As shown in the diagram, we can add additional servers to the agent, enabling seamless interactions between them. A good example is the exa-mcp-server, developed by EXA (Ishan Goswami), which allows performing web searches using EXA Searcher.</p> <p>Finally, as David Soria Parra (the creator of Model Context Protocol) noted on this X post, one MCP server can call another, allowing developers to build sophisticated, layered solutions.</p>"},{"location":"technical-writings/2025/01/20/connecting-mcp-servers-to-langgraph/#langgraph-integration-client","title":"LangGraph Integration (Client)","text":"<p>To integrate the server that I showed before\u2014or any server of your choice\u2014with LangGraph, a low code agentic framework (the best for complex workflows or fine-grained control :) ), I implemented an abstraction that allows your agents to access MCP tool servers in just a few lines of code, allowing developers to add tools, validate inputs, and manage executions with minimal effort:</p> <p><pre><code>tools = []\nasync with LangGraphMCPClient(params) as mcp:\n    tools.extend(await mcp.get_tools())\n    graph = create_react_agent(ChatOpenAI(\"gpt-4o\"), tools=tools)\n</code></pre> Additionally, you can integrate it with other ecosystems like ArcadeAI, which has a similar architecture, and enhance our agents with pre-built tools with auth (Google, X, Slack).</p> <pre><code>tools = []\n\n# Get tools from MCP\nasync with LanggraphMCPClient(server_params=server_params) as mcp_client:\n    tools.extend(await mcp_client.get_tools())\n\n     # Get tools from Arcade\n    tool_arcade_manager = ArcadeToolManager()\n    tools.extend(tool_arcade_manager.get_tools(toolkits=[\"slack\"]))\n</code></pre> <p>You can find more details here:</p> <ul> <li>The complete LangGraph-MCP integration: LangGraph MCP Client</li> <li>The core abstraction layer implementation: LangGraph MCP Manager</li> </ul> <pre><code>class LanggraphMCPClient(BaseMCPClient):\n\n    def tool_call(self, tool_name: str) -&gt; Any:\n        \"\"\"Create an asynchronous function to call a tool by its name.\n\n        Args:\n            tool_name: The name of the tool to be called.\n\n        Returns:\n            An asynchronous function that executes the tool with the provided arguments.\n        \"\"\"\n\n        async def tool_function(*args: Any, **kwargs: Any) -&gt; Any:\n            result = await self.session.call_tool(tool_name, arguments=kwargs)\n            return result\n\n        return tool_function\n\n    def wrap_tool(\n        self, tool: Any, **kwargs: Any\n    ) -&gt; StructuredTool:\n        \"\"\"Wrap a tool as a StructuredTool instance.\n\n        Args:\n            tool: The tool object to wrap.\n            **kwargs: Additional keyword arguments for tool configuration.\n\n        Returns:\n            A StructuredTool instance configured with the provided tool and arguments.\n        \"\"\"\n\n        return StructuredTool.from_function(\n            coroutine=self.tool_call(tool.name),\n            name=tool.name,\n            description=tool.description or \"No description provided.\",\n            args_schema=create_pydantic_model_from_json_schema(tool.name, tool.inputSchema),\n        )\n</code></pre>"},{"location":"technical-writings/2025/01/20/connecting-mcp-servers-to-langgraph/#conclusion","title":"Conclusion","text":"<p>In this article, I've shown you how to implement an MCP server and connect it to a custom solution on LangGraph. Please note that this is an experimental use of LangGraph with MCP and not an official or production-ready implementation (IT NEEDS TESTING and corresponding EVALUATIONS \ud83d\ude0a).</p> <p>To stay updated on MCP's advancements, check out the MCP 2025 Roadmap here. </p> <p>I hope this helps you to understand how MCP works and how you can use it to build your own agents. I'd love to hear your thoughts\u2014share your experiences in the comments below!</p>"},{"location":"technical-writings/2024/10/21/openais-swarm/","title":"OpenAI's Swarm: The Power of Right Abstractions","text":"<p>After OpenAI announced the public release of Swarm, a library designed to build reliable multi-agent systems, I decided to dive in and explore its potential by integrating it into a real-world application.</p>"},{"location":"technical-writings/2024/10/21/openais-swarm/#a-multi-agent-solution","title":"A Multi-Agent Solution","text":"<p>I adapted our productivity app at Pampa, which uses a combination of AI agents and tools to streamline various tasks such as expense tracking, meal ordering, and meeting scheduling, all in less than one hour.</p> <p>Here's a quick overview of the key aspects of the implementation.</p>"},{"location":"technical-writings/2024/10/21/openais-swarm/#1-main-agent-definition","title":"1. Main Agent Definition","text":"<p>The agent definition in Swarm is conceptually similar to our current solution, but with some key syntactic differences. I was impressed by how concise and expressive the implementation turned out to be - just 36 lines of code. This brevity demonstrates the power of Swarm's well-designed abstractions for defining multi-agent architectures.</p> <pre><code>from swarm import Agent, Swarm\n\nAGENT_SYSTEM_PROMPT = \"\"\"\n# Task\nTu nombre es Gabriela y sos un asistente de IA dise\u00f1ado para ayudar al equipo de Pampa Labs.\n\n# Guidelines\nTus respuestas deben ser:\n\n1. Amigables y accesibles, usando un tono c\u00e1lido\n2. Concisas y al grano, evitando verbosidad innecesaria\n3. \u00datiles e informativas, proporcionando informaci\u00f3n precisa\n4. Respetuosas de la privacidad del usuario y los l\u00edmites \u00e9ticos\n\nSolo puedes ayudar usando las herramientas disponibles y con pedidos que vengan de miembros del equipo. Todo lo que no se pueda responder usando las herramientas, debes decir que no puedes ayudar y disculparte.\n\"\"\"\n\nclass GabyAgent:\n    def __init__(\n        self,\n        prompt=AGENT_SYSTEM_PROMPT,\n        model_name: str = \"gpt-4o\",\n        tools: List[Callable] = [],\n    ):\n        self.agent = Agent(\n            name=\"Main Agent\",\n            instructions=prompt,\n            functions=tools,\n            model=model_name\n        )\n        self.client = Swarm()\n\n    def invoke(self, id, messages, debug=False):\n        response = self.client.run(\n            agent=self.agent,\n            messages=messages,\n            debug=debug,\n            context_variables={\"id\": id}\n        )\n        return response.messages[-1][\"content\"]\n</code></pre>"},{"location":"technical-writings/2024/10/21/openais-swarm/#2-tools","title":"2. Tools","text":"<p>Swarm's approach to defining functions is simple and flexible, relying mainly on LLM function calls.</p> <p>Since Pydantic models couldn't be used for function parameters, I adapted the tools to use docstrings for OpenAI API calls and passed the arguments explicitly to the function.</p> <pre><code>def set_meal_tool(context_variables, meal: str, date: str):\n    \"\"\"\n    Sets the meal plan for a specific date.\n\n    Args:\n        meal (str): The name of the meal.\n        date (str): The date of the meal plan.\n    \"\"\"\n    return f\"Meal plan created and sent to the provider: {meal} for {date} by team member {context_variables['id']}\"\n</code></pre> <p>As you can see, the function simply returns a string or a type that has a str representation, without needing to manage complex messages\u2014everything happens under the hood.</p> <pre><code>def get_expenses_tool(context_variables):\n    \"\"\"\n    Retrieves all pending expenses.\n    \"\"\"\n    # Mock implementation for testing purposes\n    mock_expenses = {\n        \"user1\": {\n            'expenses': [\n                {\n                    \"expense_type\": \"food\",\n                    \"date\": \"2023-05-01\",\n                    \"total_value\": 50.0,\n                    \"state\": \"pending\"\n                },\n                {\n                    \"expense_type\": \"coffee\",\n                    \"date\": \"2023-05-02\",\n                    \"total_value\": 5.0,\n                    \"state\": \"pending\"\n                }\n            ]\n        }\n    }\n    # Functions called by an Agent should return a type that has __str__\n    return f\"Expenses retrieved: {mock_expenses[context_variables['id']]['expenses']}\"\n</code></pre> <p>Here's an example of how to use these tools with the GabyAgent:</p> <pre><code>gaby_agent = GabyAgent(tools=[set_meal_tool, get_expenses_tool])\nresponse_content = gaby_agent.invoke(\n    id=\"user1\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"I want to set the meal plan for the date 2024-05-01. The meal is lasagna.\"},\n        {\"role\": \"user\", \"content\": \"I want to get the expenses\"}\n    ],\n    debug=False\n)\n</code></pre> <p>Output:</p> <pre><code>\u00a1Listo!\n1. **Comida**: $50.00 (Fecha: 2023-05-01)\n2. **Caf\u00e9**: $5.00 (Fecha: 2023-05-02)\n\nSi necesitas algo m\u00e1s, no dudes en avisarme. \ud83d\ude0a\n</code></pre>"},{"location":"technical-writings/2024/10/21/openais-swarm/#3-context-variables","title":"3. Context Variables","text":"<p>Context variables enable state management across agent interactions, allowing us to pass configuration variables like <code>id</code> for use throughout the interactions:</p> <pre><code>response = self.client.run(\n    agent=self.agent,\n    messages=messages,\n    debug=debug,\n    context_variables={\"id\": id}\n)\n</code></pre> <p>In Swarm, I couldn't find a built-in memory management feature, so if we needed to manage it, we would have to implement our own solution to maintain conversation history.</p>"},{"location":"technical-writings/2024/10/21/openais-swarm/#4-tracking","title":"4. Tracking","text":"<p>As expected of an experimental library, it offers deep debugging logging that helps in understanding agent behavior and interactions. However, it lacks built-in tracking or monitoring capabilities.</p> <p>To track requests by default, our solution uses Langsmith, which provides a detailed structure of the nodes and the actions taken at each step. I had to implement this tracking manually using decorators.</p>"},{"location":"technical-writings/2024/10/21/openais-swarm/#5-streaming","title":"5. Streaming","text":"<p>The library also supports streaming responses, which can be particularly useful for providing a more responsive user experience.</p> <pre><code>def stream(self, id, messages, debug=False):\n    stream = self.client.run(\n        agent=self.agent,\n        messages=messages,\n        debug=debug,\n        context_variables={\"id\": id},\n        stream=True\n    )\n    for chunk in stream:\n        yield chunk\n</code></pre>"},{"location":"technical-writings/2024/10/21/openais-swarm/#6-handoffs-between-agents","title":"6. Handoffs between Agents","text":"<p>I surprised by how quickly I could iterate and perform handoffs between agents. The process for defining handoffs is quite similar to Crewai's approach.</p> <p>In our use case, I was able to efficiently split tasks across different agents to divide responsibilities, such as creating a meal agent and an expenses agent.</p> <p>Here's a diagram illustrating the structure of our multi-agent system using Swarm:</p> <p></p> <p>As illustrated in the diagram, the beauty of this approach lies in its simplicity. I didn't need to modify the existing tools at all. Instead, I only had to define the transfer functions, which act as bridges between the main agent and the specialized agents. This elegant design allows for seamless task delegation and efficient collaboration between agents, all while maintaining the integrity of the individual tools and their functionalities.</p> <pre><code>meal_agent: Agent = Agent(\n    name=\"Meal Agent\",\n    instructions=\"\"\"\n    Eres un experto en planificar pedidos de comidas para el Equipo, tus respuesta deben ser claras, directas y en argentino.\n    \"\"\",\n    functions=[set_meal_tool],\n    model=\"gpt-4o\"\n)\n\ndef transfer_to_meal_agent() -&gt; Agent:\n    \"\"\"\n    Transfer control to the Meal Agent for handling meal-related tasks.\n    \"\"\"\n    return meal_agent\n\nexpenses_agent: Agent = Agent(\n    name=\"Expenses Agent\", \n    instructions=\"\"\"\n    Eres un experto en gestionar los gastos del Equipo. Tus respuestas deben ser en argentino.\n    \"\"\",\n    functions=[get_expenses_tool],\n    model=\"gpt-4o\"\n)\n\ndef transfer_to_expenses_agent() -&gt; Agent:\n    \"\"\"\n    Transfer control to the Expenses Agent for handling expenses-related tasks.\n    \"\"\"\n    return expenses_agent\n\ngaby_agent = GabyAgent(tools=[transfer_to_meal_agent, transfer_to_expenses_agent])\n</code></pre>"},{"location":"technical-writings/2024/10/21/openais-swarm/#potential-of-swarm","title":"Potential of Swarm","text":"<p>Swarm shows great promise with its approach to agent orchestration:</p> <ul> <li> <p>Simplicity and Core Focus: The design prioritizes ease of use and quick prototyping, focusing on core functionalities, allowing developers to build custom solutions without unnecessary complexity. This makes it lightweight and easy to adapt.</p> </li> <li> <p>Flexibility: It allows for the rapid definition and orchestration of multiple agents with different roles and capabilities.</p> </li> <li> <p>Experimental Nature: As an educational resource, it explores new patterns in multi-agent systems, encouraging developers to learn and experiment with agent orchestration.</p> </li> <li> <p>OpenAI Integration: Designed to integrate seamlessly with OpenAI's models and APIs, it provides a straightforward way for developers to leverage these tools.</p> </li> </ul>"},{"location":"technical-writings/2024/10/21/openais-swarm/#notebook","title":"Notebook","text":"<p>For a hands-on experience with the concepts discussed in this article, check out the accompanying Jupyter notebook:</p> <p>Swarm Framework Example Notebook</p> <p>This notebook provides a step-by-step guide to implementing a multi-agent system using OpenAI's Swarm framework, demonstrating the concepts and code snippets discussed in this article.</p>"},{"location":"technical-writings/2024/10/21/openais-swarm/#conclusion","title":"Conclusion","text":"<p>OpenAI's Swarm stands out as a library developed with well-designed abstractions to build reliable multi-agent systems.</p> <p>Have you experimented with Swarm or similar frameworks? Share your experiences in the comments below.</p>"},{"location":"technical-writings/2024/11/29/the-great-process-reset/","title":"The Great Process Reset","text":"<p>What if AI could free us from endless paperwork? Through Tukki's legal-tech journey, learn how proper digital foundations can transform administrative tedium into meaningful work.</p> <p>We've all been there. Staring at yet another form, clicking through endless screens, or jumping through bureaucratic hoops that seem to exist purely for the sake of existing. You know it's pointless. I know it's pointless. Even the person who designed it probably knows it's pointless. Yet we play by these arbitrary rules, because that's just how things are done.</p> <p>But what if they didn't have to be? What if the rise of AI could finally break this cycle of bureaucratic madness? While we might not be able to eliminate bureaucracy entirely (let's be realistic), we now have the power to transform these often senseless processes into something more bearable \u2013 maybe even efficient. </p> <p>The promise isn't just about automating paperwork. It's about fundamentally rethinking how we build technology, making it work for people rather than against them. </p>"},{"location":"technical-writings/2024/11/29/the-great-process-reset/#freeing-professionals-from-low-value-chores","title":"Freeing professionals from low-value chores","text":"<p>Traditionally, navigating bureaucracy has relied on specialists like lawyers who master tax codes, immigration consultants who understand visa requirements, and insurance experts who decode policies. Their expertise is invaluable, but a significant portion of their work often involves navigating repetitive and administratively heavy processes.</p> <p>AI is reshaping this landscape, not by sidelining these professionals, but by freeing them from routine tasks through automation and digitalization. By streamlining processes and making information more accessible, AI allows experts to focus on value-added, interesting tasks rather than spending their time on mundane admin work. This shift enables them to direct their skills toward crafting complex strategies and interpreting nuanced scenarios. </p>"},{"location":"technical-writings/2024/11/29/the-great-process-reset/#digital-foundation","title":"Digital Foundation","text":"<p>Here's the catch, though: AI isn't magic. It can't make sense of scattered paper files, inconsistent spreadsheets, or processes that exist only in your team members' heads. The irony is striking \u2013 while we debate how close we are to AGI, many of us still queue at government offices, clutching 500-page paper forms for procedures that seem to last an eternity. </p> <p>This means taking a hard look at your processes and data. Are your documents digitized and properly structured, or are they gathering dust in filing cabinets? Can your systems communicate seamlessly, or are they operating in isolation? Have you standardized your workflows, or is each department running its own show? These aren't mere IT checkboxes \u2013 these are enablers for impactful AI applications: the better the foundations on which you'll deploy AI, the more impact it can achieve. A poor foundation will constrain AI projects to live within silos, just as many teams do operate nowadays in large organizations.</p> <p>This need for strong foundations is exactly what Tukki embraced when building their legal-tech platform. Focused on transforming the US immigration experience, Tukki demonstrates how starting with the basics - organizing workflows and digitizing processes - can lay the groundwork for effective AI implementation.</p>"},{"location":"technical-writings/2024/11/29/the-great-process-reset/#a-real-life-example-tukki","title":"A real life example: Tukki","text":"<p>Tukki.ai is a legal-tech startup that's bringing a premium US immigration experience to all by uniting tech and lawyers under the same roof. If you want to move to or work in the US, you can hire Tukki to access their platform which powers their top notch paralegal team and an immigration attorney\u2014designated to you based on your unique case. They charge the same as a traditional law firm that handles everything manually, but you get to see every step, advance, and task at a glance. No more losing files in emails, chasing lawyers, or endless back-and-forth on Word docs.</p> <p>When Ramiro Roballos and Saveliy Vasilev, the founders of Tukki, launched the company, the temptation was to tinker with AI from day 1\u2014everyone was trying to surf the AI wave. However, they knew that without proper foundations, they'll get only so far. From their former consulting careers, Ramiro and Saveliy gained insights into applying AI in organizations\u2014big and small. </p> <p>A key learning was: If you're aiming for a large and sustainable impact with AI, you better get the technological basics right first. Only with a sound foundation can you feed the models and consume their outputs effectively. Getting the basics right means capturing most of the data flows in your digital medium of choice, as long as it is easily consumable for a data science team. </p> <p> Tukki's platform showing how digital tasks are handled</p> <p>Consider the extremes of the technologically empowered spectrum: If your business runs on exchanging powerpoints and excel sheets via emails, then it'll be tough to improve these processes. If, at the other extreme, you have a platform that captures all your key workflows and all the data lives inside this platform, then extracting value from the historical data is just around the corner for your company. Tukki\u2019s tech strategy was simple: build software that would a) organize everyone's work, and b) progressively move processes over to be handled in their platform. They call processes that still happen outside of their tech \"off platform.\"</p> <p>This approach has already paid dividends:</p> <ul> <li>The organization of legal work turned out to be a large undertaking in itself, efficiencies gained simply by organizing and documenting everything in one place made the processes way better straight off the bat\u2014without AI!</li> <li>As soon as they make a release that allows a process to be handled on-platform, the users begin to generate useful data. This allowed Tukki\u2019s team to introduce some AI in a very natural way. In fact, they recently updated some of these models by taking a larger dataset into account, which further enhanced the efficiencies.</li> </ul>"},{"location":"technical-writings/2024/11/29/the-great-process-reset/#wrapping-up","title":"Wrapping up","text":"<p>As Tukki's story shows us, AI is not a magic wand that solves al our problems. It is an accelerator, and the key to using it wisely is building the right foundation first. Before you can unlock AI's potential, your processes need to be organized and digital.  It's straightforward: build your digital foundation, and AI becomes the accelerator that transforms bureaucratic paperwork into high-value work. No shortcuts \u2013 just strategic groundwork to become truly LLM native.</p>"},{"location":"technical-writings/archive/2025/","title":"2025","text":""},{"location":"technical-writings/archive/2024/","title":"2024","text":""}]}